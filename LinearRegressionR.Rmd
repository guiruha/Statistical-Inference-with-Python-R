---
title: "LinearRegression"
author: "Guillem"
date: "11 March 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Regresión Lineal Simple

Cálculos a mano de los coeficientes

```{r}
sal <- c(1.8, 2.2, 3.5, 4.0, 4.3, 5.0)
tension <- c(100, 98, 110, 110, 112, 120)
(media_sal <- mean(sal))
(media_tension <- mean(tension))
(var_sal <- var(sal))
(cov_salten <- cov(sal, tension))
(b1 <- cov_salten/var_sal)
(b0 <- media_tension - b1*media_sal)
```
La recta de regresión será: $$\hat{y} = 86.3707865 + 6.33535 · x$$

Ahora lo calculamos directamente

```{r}
lm(tension ~ sal)
```
Vamos a comprobar las propiedades que cumple la regresión por mínimos cuadrados

- La **recta de regresión** pasa por el vector medio $(\bar{x}, \bar{y})$


```{r}
(round(media_tension - b0 - b1*media_sal, 6))
```
- La media de los valores estimados a partir de la **recta de regresión** es igual a la media de los observados $(y_i, \bar{y})$

```{r}
tension_estimada <- b0 + b1*sal
(mean(tension_estimada)-mean(tension))
```

- La estimación de la varianza para los datos del ejemplo anterior es la siguiente:

```{r}
errores <- tension_estimada - tension
SSE <- sum(errores**2)
n = length(sal)
(estimacion_varianza <- SSE/(n-2))
```
Entonces tenemos qu eel valor aproximado o estimado de $\sigma_E ^2$ es 5.4364

Calulemos las variabilidades anteriores y el **coeficiente de determinación** par alos datos del ejemplo:

-**Variabilidad total**:
```{r}
(SST <- sum((tension - media_tension)^2))
```
- **Variabilidad de la regresión**

```{r}
(SSR = sum((tension_estimada - media_tension)^2))
```

- **Variabilidad del error:**

```{r}
(SSE <- sum((tension- tension_estimada)**2))
(round(SST-SSR-SSE, 6)) # SST = SSR + SSE
```

El coeficiente de determinación $R^2$ será:

```{r}
(R2 <- SSR/SST)
```
Otra manera de calcularlo es:

```{r}
(R2 <- var(tension_estimada)/var(tension))
```
Podemos concluir que la regresión explica un 93.44% de la variabilidad de los datos

De forma automática podemos ver el $R^2$

```{r}
summary(lm(tension ~ sal))$r.squared
```

Comprobamos para los atos d enuestro ejemplo si los errores siguen una distribución normal usando el test de **Kolmogorov-Smirnov-Lilliefors**:

```{r}
library(nortest)
lillie.test(errores)
```
Como el $p_value$ es grande, podemos concluir que no teenmos evidencias suficientes para rechazar que los errores siguen una distribución normal

Hallemos los intervalos de confianza para el 95% de confianza para los parámetros $\beta_1$ y $\beta_0$

```{r}
alpha <- 0.05
S <- sqrt(estimacion_varianza)
extremo_izb1 <- b1-qt(1-alpha/2, n-2)*S/(sd(sal)*sqrt(n-1))
extremo_derb1 <- b1+qt(1-alpha/2, n-2)*S/(sd(sal)*sqrt(n-1))
(c(extremo_izb1, extremo_derb1))
```
```{r}
alpha <- 0.05
S <- sqrt(estimacion_varianza)
extremo_izb1 <- b0-qt(1-alpha/2, n-2)*S*sqrt(1/n+media_sal^2/((n-1)*var(sal)))
extremo_derb1 <- b0+qt(1-alpha/2, n-2)*S*sqrt(1/n+media_sal^2/((n-1)*var(sal)))
(c(extremo_izb1, extremo_derb1))
```


En R ya existe una función que automatiza estos cálculos
```{r}
confint(lm(tension~sal), level = 0.95)
```

Hallemos un intervalor de  confianza par aun nivel de sal $x_0 = 4.5$ para los parámetros $\mu_{y|4.5}$ y $y_0$ al 95% de confianza:

- Intervalo de confianza para $\mu_{y|4.5}$
```{r}
alpha <- 0.05
x0 <- 4.5
y0_estimado <- b0 + b1*x0
extremo_izmux0 <- y0_estimado - qt(1-alpha/2, n-2)*S*sqrt(1/n+(x0-media_sal)^2/((n-1)*var(sal)))
extremo_dermux0 <-  y0_estimado + qt(1-alpha/2, n-2)*S*sqrt(1/n+(x0-media_sal)^2/((n-1)*var(sal)))
(c(extremo_izmux0, extremo_dermux0))
```

- Intervalo de confianza para $\hat{y}$ para $x_0 = 4.5$
```{r}
extremo_izmux0 <- y0_estimado - qt(1-alpha/2, n-2)*S*sqrt(1+ 1/n+(x0-media_sal)^2/((n-1)*var(sal)))
extremo_dermux0 <-  y0_estimado + qt(1-alpha/2, n-2)*S*sqrt(1 + 1/n+(x0-media_sal)^2/((n-1)*var(sal)))
(c(extremo_izmux0, extremo_dermux0))
```
Para hallar el intervalo de confianza para el parámetro $\mu_{y|x_0}$ al $100 · (1 - \alpha)%$ de confianza hay que usar la función **predict.lm** de la forma siguiente:

**newdata <- data.frame(x = x0)**
**predict.lm(lm(y~x), newdata, interval = "confidence", level = nivel.confianza)**

Para el parámetro $y_0$ hay que usar la misma función pero cambiando el parámetro interval al valor predicition:

**newdata <- data.frame(x = x0)**
**predict.lm(lm(y~x), newdata, interval = "prediction", level = nivel.confianza)**

Hallamos los intervalos de confianza para los parámetros $\mu_{Y|4.5}$ y $y_0$ al 95% de confianza: 
```{r}
newdata <- data.frame(sal = 4.5)
predict.lm(lm(tension~sal), newdata, interval = "confidence", level = 0.95)

predict.lm(lm(tension~sal), newdata, interval = "prediction", level = 0.95)
```
### Contrastes de hipotesis de B1

$$
\left\{
\begin{array}{ll}
H_0: & \beta_1 = 0 \\\
H_1: & \beta_1 \neq 0
\end{array}
\right.
$$

```{r}
(t0 <- b1/(S/sd(sal)*sqrt(n-1)))
(p <- 2*pt(abs(t0), n-2, lower.tail = FALSE))
```

Para realizar el contraste anterior se puede mirar el summary de lm

```{r}
summary(lm(tension ~ sal))
```
### Regresión lineal múltiple

Vamos a calcular los parámetros $\beta$ de una regresión multiple con la fórmula matricial $\hat\beta = (X^t · X)^{-1} · (X^t · y)$

```{r}
X = matrix(c(1, 78, 48.2, 2.75, 29.5, 1, 69, 45.5, 2.15, 26.3, 1, 77, 46.3, 4.41, 32.2, 1, 88, 49, 5.52, 36.5, 1, 67, 43, 3.21, 27.2, 1, 80, 48, 4.32, 27.7, 1, 74, 48, 2.31, 28.3, 1, 94, 53, 4.3, 30.3, 1, 102, 58, 3.71, 28.7), nrow = 9, byrow = TRUE)

y = c(57.5, 52.8, 61.3, 67, 53.5, 62.7, 56.2, 68.5, 69.2)

(estimaciones <- solve(t(X)%*%X)%*%(t(X)%*%y))
```
En conclusión la función lineal de regresión es:
$\hat{y} = 7.1475 + 0.100x_1 + 0.7264x_2 + 3.0758x_3 - 0.030x_4$

Automáticamente podemos calcularlo con lm

```{r}
lm(y ~ X[,2]+X[,3]+X[,4]+X[,5])
summary(lm(y ~ X[,2]+X[,3]+X[,4]+X[,5]))
```

Verifiquemos las propiedades de la regresión lineal:

- La función de regresión pasa por el medio $(\bar{x}_1, \bar{x}_2, ... , \bar{x}_k, \bar{y})$

```{r}
valores_medios <- apply(X[,1:5], 2, mean)
round(mean(y) - t(estimaciones)%*%valores_medios, 6)
```
- La media de los valores estimados es igual a la media de los observados:

```{r}
valores_estimados <- X%*%estimaciones
round(mean(y) - mean(valores_estimados), 6)
```
- Los errores $(e_i)_{i = 1, .., n}$ tienen media 0 y varianza $\tilde{S}_e^2 = \frac{SS_E}{n-1}$

```{r}
errores <- y - valores_estimados
round(mean(errores))
```

```{r}
SSE <- sum(errores ^ 2)
n <- dim(X)[1]
var(errores) - SSE/(n-1)
```

Calculemos ahora las variabilidades y el coeficiente de determinación para los datos del ejemplo trabajado:

- Variabilidad total:
```{r}
(SST <- sum((y - mean(y))^2))
```
- Variabilidad de la regresión:
```{r}
(SSR <- sum((valores_estimados - mean(y))^2))
```
- Variabilidad de los errores:
```{r}
(SSE <- sum((valores_estimados - y)^2))
```
Ahora comprobamos que la **variabilidad total** se descompone d ela suma de la **variabilidad de la regresión** y la **variabilidad del error**:

```{r}
round(SST - SSR - SSE, 6)
```

El coeficiente de determinación será:

```{r}
(R2 <- SSR/SST)
(R2 <- var(valores_estimados)/var(y))
```

```{r}
summary(lm(y ~ X[,2]+X[,3]+X[,4]+X[,5]))$r.squared
```

Calculemos ahora el **coeficiente de determinación ajustado** 

```{r}
k <- dim(X)[2]-1
(R2_adj <- 1 - (1 - R2)*(n-1)/(n-k-1))
```


```{r}
summary(lm(y ~ X[,2]+X[,3]+X[,4]+X[,5]))$adj.r.squared
```

```{r}
(summary(lm(y ~ X[,2]))$adj.r.squared)
(summary(lm(y ~ X[,2]+X[,3]))$adj.r.squared)
(summary(lm(y ~ X[,2]+X[,3]+X[,4]))$adj.r.squared)
(summary(lm(y ~ X[,2]+X[,3]+X[,4]+X[,5]))$adj.r.squared)
```

Observamos que la mejor $R_{adj}^2$ se consigue con los coeficientes $\beta_1$, $\beta_2$ y $\beta_3$, por lo que el modelo óptimo solo utilizaria estos coeficientes

También podemos usar el método de AIC (Akaike's information criterion)

```{r}
(AIC(lm(y~X[,2]+X[,3]+X[,4]+X[,5])))
(AIC(lm(y~X[,2]+X[,3]+X[,4])))
(AIC(lm(y~X[,2]+X[,3])))
(AIC(lm(y~X[,2])))
```

Escogemos el modelo que tenga un AIC más bajo por tanto la conclusión es la misma que con el R cuadrado ajustado ($R_{adj}^2$).

Una forma similar a AIC es aplicar el método BIC (Bayessian information criterion):

```{r}
(BIC(lm(y~X[,2]+X[,3]+X[,4]+X[,5])))
(BIC(lm(y~X[,2]+X[,3]+X[,4])))
(BIC(lm(y~X[,2]+X[,3])))
(BIC(lm(y~X[,2])))
```

Al igual que con AIC escogemos el BIC más bajo por tanto la conclusión es la misma que tenemos con AIC y R cuadrado ajustado ($R_{adj}^2$)

Veamos si los errores de los datos del ejmplo se distribuyen normalmente usando el test de **Kolmogorov-Smirnov-Lilliefors**

```{r}
lillie.test(errores)
```

Como el $p_{value}$ es muy grande, concluimos que no hay evidencias suficientes para rechazar que los errores se distribuyen en una normal

La estimación de la varianza común $S^2$ será:

```{r}
(S2 <- SSE/(n-k-1))
```

La estimación de la matriz de covarianzas de los estimadores $b_1, b_2, b_3, b_4$ es la siguiente:

```{r}
S2*solve(t(X)%*%X)
```

En la matriz anterior, podemos observar que el estimador con más varianza sería $b_0$ seguido de $b_3$.

Las estimaciones de los errores estándar de los estimadores son:

```{r}
(errores_estandar <- sqrt(S2*diag(solve(t(X)%*%X))))
```

- Intervalo de confianza para $b_0$ al 95% de confianza:
```{r}
alpha = 0.05
c(estimaciones[1] - qt(1-alpha/2, n-k-1)*errores_estandar[1],
  estimaciones[1] + qt(1-alpha/2, n-k-1)*errores_estandar[1])
```

```{r}
alpha = 0.05
c(estimaciones[2] - qt(1-alpha/2, n-k-1)*errores_estandar[2],
  estimaciones[2] + qt(1-alpha/2, n-k-1)*errores_estandar[2])
```
```{r}
alpha = 0.05
c(estimaciones[3] - qt(1-alpha/2, n-k-1)*errores_estandar[3],
  estimaciones[3] + qt(1-alpha/2, n-k-1)*errores_estandar[3])
```

```{r}
alpha = 0.05
c(estimaciones[4] - qt(1-alpha/2, n-k-1)*errores_estandar[4],
  estimaciones[4] + qt(1-alpha/2, n-k-1)*errores_estandar[4])
```

```{r}
alpha = 0.05
c(estimaciones[5] - qt(1-alpha/2, n-k-1)*errores_estandar[5],
  estimaciones[5] + qt(1-alpha/2, n-k-1)*errores_estandar[5])
```
Mucho más rápido se hace con confint de lm

```{r}
confint(lm(y~X[,2]+X[,3]+X[,4]+X[,5]), level = 0.95)
```

Hallemos un intervalo de confianza para los datos del ejemplo anterior x10 = 75, x20 = 50, x30 = 4 y x40 = 30, el inervalo de ocnfianza para le parámetro $\mu_{Y|x_10 = 75, x_20 = 50, x_30 = 4, x_40 = 30}$ al 95 % de confianza

```{r}
alpha = 0.05
x0 = c(1, 75, 50, 4, 30)
y0_estimado = sum(estimaciones*x0)
c(y0_estimado - qt(1-alpha/2, n-k-1)*sqrt(S2*(t(x0)%*%solve(t(X)%*%X)%*%x0)), y0_estimado + qt(1-alpha/2, n-k-1)*sqrt(S2*(t(x0)%*%solve(t(X)%*%X)%*%x0)))
```

El intervalo de confianza para el parámetro $y_0$ al 95% de confianza es el siguiente:

```{r}
c(y0_estimado - qt(1-alpha/2, n-k-1)*sqrt(S2*(1+ t(x0)%*%solve(t(X)%*%X)%*%x0)), y0_estimado + qt(1-alpha/2, n-k-1)*sqrt(S2*(1+ t(x0)%*%solve(t(X)%*%X)%*%x0)))
```

```{r}
newdata <- data.frame(x1 = 75, x2 = 50, x3 = 4, x4 = 30)
x1 <- X[, 2]
x2 <- X[, 3]
x3 <- X[, 4]
x4 <- X[, 5]
predict.lm(lm(y~x1+x2+x3+x4), newdata, interval = "confidence", level = 0.95)

predict.lm(lm(y~x1+x2+x3+x4), newdata, interval = "prediction", level = 0.95)
```

